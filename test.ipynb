{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.tests import *\n",
    "from batchstream.history.base.history_manager import HistoryManager\n",
    "from batchstream.monitoring.pipeline.model_monitoring_pipeline import ModelMonitoringPipeline\n",
    "from batchstream.monitoring.pipeline.steps.batch.evidently_monitoring_step import EvidentlyMonitoringStep\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "from batchstream.batch_monitoring_strategy.dummy_monitoring_strategy import DummyMonitoringStrategy\n",
    "from batchstream.retraining_strategy.dummy_retraining_strategy import DummyRetrainingStrategy \n",
    "from batchstream.model_comparers.batch_comparer import BatchModelComparer\n",
    "from batchstream.model_comparers.shadow_comparer import ShadowOnlineComparer\n",
    "from batchstream.pipelines.batch.batch_pipeline import BatchPipeline\n",
    "from batchstream.estimators.sklearn_estimator import SklearnEstimator\n",
    "from batchstream.detectors.base.detector import DriftDetector\n",
    "from batchstream.experiment.experiment import StreamExperiment\n",
    "from river.metrics import Accuracy, ROCAUC\n",
    "from batchstream.evaluation.model_evaluation_pipeline import ModelEvaluationPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "history = HistoryManager()\n",
    "logger_factory = LoggerFactory('test-2220')\n",
    "\n",
    "\n",
    "### INPUT DRIFT DETECTION\n",
    "# Detector 1.1 - Data Drift\n",
    "data_drift_test_suite = TestSuite(tests=[\n",
    "   DataDriftTestPreset(),\n",
    "])\n",
    "d1 = DummyMonitoringStrategy(n_curr=120, n_ref=120)\n",
    "ev1 = EvidentlyMonitoringStep(data_drift_test_suite, d1, logger_factory, min_instances=240, clock=120, name='data_drift_eval')\n",
    "\n",
    "# Detector 1.2 - Target Drift\n",
    "target_drift = TestSuite(tests=[\n",
    "    TestColumnDrift(column_name='target'),\n",
    "])\n",
    "d2 = DummyMonitoringStrategy(n_curr=120, n_ref=120, type='target')\n",
    "ev2 = EvidentlyMonitoringStep(target_drift, d2, logger_factory, min_instances=240, clock=120, name='target_drift_eval')\n",
    "\n",
    "input_monitoring = ModelMonitoringPipeline([(ev1._name, ev1), (ev2._name, ev2)])\n",
    "input_drift_retraining_strategy = DummyRetrainingStrategy(n_last_retrain=120, n_last_test=0)\n",
    "input_detector = DriftDetector(input_monitoring, input_drift_retraining_strategy)\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OUTPUT (PERFORMANCE) DRIFT DETECTION\n",
    "# Detector 2.1 - Performance Drift\n",
    "\n",
    "performance_drift = TestSuite(tests=[\n",
    "    TestPrecisionScore(),\n",
    "    TestRecallScore(),\n",
    "    TestF1Score(),\n",
    "    TestAccuracyScore()\n",
    "])\n",
    "d3 = DummyMonitoringStrategy(n_curr=120, n_ref=120, type='prediction')\n",
    "ev3 = EvidentlyMonitoringStep(performance_drift, d3, logger_factory, min_instances=360, clock=120, name='performance_drift_eval')\n",
    "\n",
    "output_monitoring = ModelMonitoringPipeline([(ev3._name, ev3)])\n",
    "output_drift_retraining_strategy = DummyRetrainingStrategy(n_last_retrain=120, n_last_test=0)\n",
    "output_detector = DriftDetector(output_monitoring, output_drift_retraining_strategy)\n",
    "###\n",
    "\n",
    "### Models comparison (after retraining)\n",
    "#model_comparer = BatchModelComparer()\n",
    "model_comparer = ShadowOnlineComparer(n_online=20)\n",
    "###\n",
    "\n",
    "\n",
    "### Model's Performance Evaluation\n",
    "acc = Accuracy()\n",
    "roc_auc = ROCAUC()\n",
    "eval_pipe = ModelEvaluationPipeline(metric_steps=[\n",
    "    ('accuracy', acc),\n",
    "    ('roc_auc', roc_auc)\n",
    "])\n",
    "###\n",
    "\n",
    "\n",
    "### Model composition\n",
    "sklearn_batch_classifier = SklearnEstimator(Pipeline([('rf', RandomForestClassifier())]))\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_detector=input_detector,\n",
    "    output_drift_detector=output_detector,\n",
    "    history=history,\n",
    "    logger_factory=logger_factory,\n",
    "    model_comparer=model_comparer,\n",
    "    min_samples_retrain=120,\n",
    "    min_samples_first_fit=240\n",
    ")\n",
    "###\n",
    "\n",
    "### Experiment\n",
    "experiment = StreamExperiment(batch_pipeline, eval_pipe, logger_factory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_breast_cancer(return_X_y=True)\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.metrics import Accuracy\n",
    "from river import utils\n",
    "\n",
    "Y_true =    [1, 0, 1, 0, 1, 0, 0]\n",
    "Y_predict = [1, 1, 0, 0, 1, 1, 1]\n",
    "\n",
    "\n",
    "roll_acc = utils.Rolling(Accuracy(), window_size=3)\n",
    "for y_t, y_p in zip(Y_true, Y_predict):\n",
    "    print(roll_acc.update(y_t, y_p).get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "\n",
    "\n",
    "\n",
    "data_drift_test_suite = TestSuite(tests=[\n",
    "    DataDriftTestPreset(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drift_test_suite._inner_suite.context.tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.tests.base_test import Test\n",
    "\n",
    "t: Test = data_drift_test_suite._inner_suite.context.tests[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchstream.pipelines.base.stream_pipeline import StreamPipeline\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "\n",
    "\n",
    "class CombinationPipeline(StreamPipeline):\n",
    "\n",
    "    def __init__(self, members: List[StreamPipeline], combiner: object):\n",
    "        self._members = members\n",
    "        self._combiner = combiner\n",
    "\n",
    "    def handle(self, x, y) -> int:\n",
    "        return self._combiner.combine(x, y, self._members)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.tests import *\n",
    "from batchstream.history.base.history_manager import HistoryManager\n",
    "from batchstream.monitoring.pipeline.model_monitoring_pipeline import ModelMonitoringPipeline\n",
    "from batchstream.monitoring.pipeline.steps.batch.evidently_monitoring_step import EvidentlyMonitoringStep\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "from batchstream.batch_monitoring_strategy.simple_monitoring_strategy import SimpleMonitoringStrategy\n",
    "from batchstream.retraining_strategy.simple_retraining_strategy import SimpleRetrainingStrategy \n",
    "from batchstream.model_comparers.batch_comparer import BatchModelComparer\n",
    "from batchstream.model_comparers.shadow_comparer import ShadowOnlineComparer\n",
    "from batchstream.pipelines.batch.batch_pipeline import BatchPipeline\n",
    "from batchstream.estimators.sklearn_estimator import SklearnEstimator\n",
    "from batchstream.detectors.base.drift_handler import DriftHandler\n",
    "from batchstream.experiment.experiment import StreamExperiment\n",
    "from river.metrics import Accuracy, ROCAUC\n",
    "from batchstream.evaluation.river_evaluation_pipeline import RiverEvaluationPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory('test-2218')\n",
    "retraining_strategy = SimpleRetrainingStrategy(\n",
    "  n_last_retrain=500, n_last_test=0)\n",
    "\n",
    "\n",
    "### INPUT DRIFT DETECTION\n",
    "# Detector 1.1 - Data Drift\n",
    "data_drift_test_suite = TestSuite(tests=[\n",
    "DataDriftTestPreset(),\n",
    "])\n",
    "d1 = SimpleMonitoringStrategy(n_curr=120, n_ref=120)\n",
    "ev1 = EvidentlyMonitoringStep(data_drift_test_suite, d1, logger_factory, min_instances=240, clock=120, name='data_drift_eval')\n",
    "\n",
    "# Detector 1.2 - Target Drift\n",
    "target_drift = TestSuite(tests=[\n",
    "    TestColumnDrift(column_name='target'),\n",
    "])\n",
    "d2 = SimpleMonitoringStrategy(n_curr=120, n_ref=120, type='target')\n",
    "ev2 = EvidentlyMonitoringStep(target_drift, d2, logger_factory, min_instances=240, clock=120, name='target_drift_eval')\n",
    "\n",
    "input_monitoring = ModelMonitoringPipeline([(ev1._name, ev1), (ev2._name, ev2)])\n",
    "input_detector = DriftHandler(input_monitoring, retraining_strategy)\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OUTPUT (PERFORMANCE) DRIFT DETECTION\n",
    "# Detector 2.1 - Performance Drift\n",
    "\n",
    "performance_drift = TestSuite(tests=[\n",
    "    TestPrecisionScore(),\n",
    "    TestRecallScore(),\n",
    "    TestF1Score(),\n",
    "    TestAccuracyScore()\n",
    "])\n",
    "d3 = SimpleMonitoringStrategy(n_curr=500, n_ref=500, type='prediction')\n",
    "ev3 = EvidentlyMonitoringStep(performance_drift, d3, logger_factory,\n",
    "  min_instances=1000, clock=500, name='performance_drift_eval')\n",
    "\n",
    "# Output Drift Handler (Performance Drift + Retraining Strategy)\n",
    "output_monitoring = ModelMonitoringPipeline([(ev3._name, ev3)])\n",
    "\n",
    "output_drift_handlers = [\n",
    "  DriftHandler(output_monitoring, retraining_strategy)\n",
    "]\n",
    "###\n",
    "\n",
    "### Models comparison (after retraining)\n",
    "#model_comparer = BatchModelComparer()\n",
    "model_comparer = ShadowOnlineComparer(n_online=100)\n",
    "###\n",
    "\n",
    "\n",
    "### Model's Performance Evaluation\n",
    "acc = Accuracy()\n",
    "roc_auc = ROCAUC()\n",
    "eval_pipe = RiverEvaluationPipeline(metric_steps=[\n",
    "    ('accuracy', acc),\n",
    "    ('roc_auc', roc_auc)\n",
    "])\n",
    "###\n",
    "\n",
    "\n",
    "### Model composition\n",
    "logger_factory = LoggerFactory(experiment_id='rf_exp')\n",
    "Pipeline([('rf', RandomForestClassifier(max_depth=10))])\n",
    "sklearn_batch_classifier = SklearnEstimator()\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.metrics import Accuracy, MacroF1, MicroF1\n",
    "from river.utils import Rolling\n",
    "\n",
    "\n",
    "\n",
    "window_size = 1000\n",
    "eval_pipe = RiverEvaluationPipeline(metric_steps=[\n",
    "  (f'acc_preq_{window_size}', Rolling(Accuracy(), window_size)),\n",
    "  (f'micro_f1_preq_{window_size}', Rolling(MicroF1(), window_size)),\n",
    "  (f'macro_f1_preq_{window_size}', Rolling(MacroF1(), window_size))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchstream.pipelines.stream.model_river_pipeline import RiverPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.forest import ARFClassifier\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river.datasets import synth\n",
    "from river import tree\n",
    "from river import ADWIN\n",
    "from river import naive_bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.ensemble import SRPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.forest import ARFClassifier\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='arf_exp')\n",
    "arf_model = ARFClassifier(seed=42, leaf_prediction=\"mc\")\n",
    "arf_pipe = RiverPipeline(arf_model)\n",
    "arf_experiment = StreamExperiment(arf_pipe, eval_pipe, logger_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tree.HoeffdingTreeClassifier(grace_period=50, delta=0.01, nominal_attributes=['age', 'car', 'zipcode'])\n",
    "srp_model = ensemble.SRPClassifier(model=base_model, n_models=3, seed=42)\n",
    "srp_pipe = RiverPipeline(srp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree import HoeffdingAdaptiveTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "hat_model = HoeffdingAdaptiveTreeClassifier(\n",
    "  grace_period=100,\n",
    "  delta=1e-5,\n",
    "  leaf_prediction='nb',\n",
    "  nb_threshold=10,\n",
    "  seed=42\n",
    ")\n",
    "hat_pipe = RiverPipeline(hat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import naive_bayes\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='nb_exp')\n",
    "nb_model =  naive_bayes.GaussianNB()\n",
    "nb_pipe = RiverPipeline(nb_model)\n",
    "nb_experiment = StreamExperiment(nb_pipe, eval_pipe, logger_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.tests import *\n",
    "\n",
    "\n",
    "\n",
    "### INPUT DRIFT DETECTION\n",
    "# Detector 1.1 - Data Drift\n",
    "data_drift_test_suite = TestSuite(tests=[\n",
    "  DataDriftTestPreset(),\n",
    "])\n",
    "d1 = DummyMonitoringStrategy(n_curr=500, n_ref=500, type='data')\n",
    "ev1 = EvidentlyMonitoringStep(data_drift_test_suite, d1, logger_factory,\n",
    "  min_instances=1000, clock=500, name='data_drift_eval'\n",
    ")\n",
    "\n",
    "# Detector 1.2 - Target Drift \n",
    "target_drift = TestSuite(tests=[\n",
    "    TestColumnDrift(column_name='target'),\n",
    "])\n",
    "d2 = DummyMonitoringStrategy(n_curr=500, n_ref=500, type='target')\n",
    "ev2 = EvidentlyMonitoringStep(target_drift, d2, logger_factory,\n",
    "  min_instances=1000, clock=500, name='target_drift_eval'\n",
    ")\n",
    "\n",
    "# Input Drift Handler (Data + Target Drift + Retraining Strategy)\n",
    "input_monitoring = ModelMonitoringPipeline([(ev1._name, ev1), (ev2._name, ev2)])\n",
    "input_drift_retraining_strategy = DummyRetrainingStrategy(\n",
    "  n_last_retrain=500, n_last_test=0\n",
    ")\n",
    "input_drift_handlers = [\n",
    "  DriftHandler(input_monitoring, input_drift_retraining_strategy)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StreamExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='rf_exp_all_drifts')\n",
    "rf_model = Pipeline([('rf', RandomForestClassifier(max_depth=10))])\n",
    "sklearn_batch_classifier = SklearnEstimator(rf_model)\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=1500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='lr_exp_all_drifts')\n",
    "lr_model = Pipeline([('lr', LogisticRegression())])\n",
    "sklearn_batch_classifier = SklearnEstimator(lr_model)\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=1500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='nb_exp_all_drifts')\n",
    "nb_model = Pipeline([('nb', GaussianNB())])\n",
    "sklearn_batch_classifier = SklearnEstimator(nb_model)\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=1500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='xgb_exp_all_drifts')\n",
    "xgb_model = Pipeline([('xgb', XGBClassifier())])\n",
    "sklearn_batch_classifier = SklearnEstimator(xgb_model)\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=1500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'1': [1, 2, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dataset'] = 'x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = range(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO, TextIOWrapper\n",
    "from zipfile import ZipFile\n",
    "from scipy.io.arff import loadarff\n",
    "from os import path\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "\n",
    "def get_covtype_dataset(data_path='./data') -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    zip_path = path.join(data_path, 'COVTYPE/covtypeNorm.arff.zip')\n",
    "    with (ZipFile(zip_path, 'r')) as zfile:\n",
    "        in_mem_fo = TextIOWrapper(BytesIO(zfile.read('covtypeNorm.arff')), encoding='ascii')\n",
    "        data = loadarff(in_mem_fo)\n",
    "        df = pd.DataFrame(data[0])\n",
    "        to_convert_df = df.select_dtypes([object])\n",
    "        to_convert_col_names = to_convert_df.columns\n",
    "        df[to_convert_col_names] = to_convert_df.stack().str.decode('utf-8').unstack()\n",
    "        class_col = df.pop('class').replace(['noad', 'ad'], [0, 1])\n",
    "        df['target'] = class_col.copy()\n",
    "        df.loc[:, \"Wilderness_Area1\":] = df.loc[:, \"Wilderness_Area1\":].astype(str).astype(int)\n",
    "        df['dataset'] = 'covtypeNorm'\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_covtype_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from batchstream.utils.visualization import visualize_results\n",
    "from batchstream.utils.reading_logs import load_drift_history, get_metrics_vals\n",
    "\n",
    "\n",
    "\n",
    "out_dir = Path(r'D:\\Studia\\praca\\out\\20230505_1848')\n",
    "dir_list = [f for f in out_dir.resolve().glob('*') if not f.is_file()]\n",
    "for d in dir_list:\n",
    "    drift_hist = load_drift_history(str(d))\n",
    "    res = get_metrics_vals(str(d))\n",
    "    print(str(d))\n",
    "    visualize_results(res, drift_hist, dataset_name='COVTYPE', metrics=['acc'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift features nie występuje\n",
    "# Drift perfromance też nie (?) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from batchstream.utils.visualization import visualize_results\n",
    "from batchstream.utils.reading_logs import load_drift_history, get_metrics_vals\n",
    "\n",
    "\n",
    "\n",
    "out_dir = Path(r'D:\\Studia\\praca\\out\\20230506_1423')\n",
    "dir_list = [f for f in out_dir.resolve().glob('*') if not f.is_file()]\n",
    "for d in dir_list:\n",
    "    drift_hist = load_drift_history(str(d))\n",
    "    res = get_metrics_vals(str(d))\n",
    "    print(str(d))\n",
    "    visualize_results(res, drift_hist, dataset_name='elec', metrics=['acc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rbf 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from batchstream.utils.visualization import visualize_results\n",
    "from batchstream.utils.reading_logs import load_drift_history, get_metrics_vals\n",
    "from utils.parse_metadata.parse_log_metadata import print_info\n",
    "\n",
    "\n",
    "\n",
    "out_dir = Path(r'D:\\Studia\\praca\\out\\20230509_1222_rbf66')\n",
    "dir_list = [f for f in out_dir.resolve().glob('*') if not f.is_file()]\n",
    "for d in dir_list:\n",
    "    print(str(d))\n",
    "    print_info(str(d))\n",
    "    drift_hist = load_drift_history(str(d))\n",
    "    res = get_metrics_vals(str(d))\n",
    "    if res is None: continue\n",
    "    visualize_results(res, drift_hist, dataset_name='rbf0.66', metrics=['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from batchstream.utils.visualization import visualize_results\n",
    "from batchstream.utils.reading_logs import load_drift_history, get_metrics_vals\n",
    "from utils.parse_metadata.parse_log_metadata import print_info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out_dir = Path(r'D:\\Studia\\praca\\out\\20230505_2021_elec')\n",
    "dir_list = [f for f in out_dir.resolve().glob('*') if not f.is_file()]\n",
    "for d in dir_list:\n",
    "    print(str(d))\n",
    "    print_info(d)\n",
    "    replacement_hist = get_replacement_hist(str(d))\n",
    "    drift_hist = load_drift_history(str(d))\n",
    "    res = get_metrics_vals(str(d))\n",
    "    if res is None: continue\n",
    "    visualize_results(res, drift_hist, dataset_name='elec', replacement_hist=replacement_hist, metrics=['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.first.rf_all_evidently import get_rf_all_evidently_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset('elec')\n",
    "exp = get_rf_all_evidently_exp(suffix='0000300000')\n",
    "\n",
    "exp.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.first.online.online import get_arf_exp\n",
    "from utils.read_data.get_dataset import get_dataset\n",
    "\n",
    "df = get_dataset('rbf66')\n",
    "exp = get_arf_exp(suffix='example')\n",
    "\n",
    "exp.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import sketch\n",
    "\n",
    "cms = sketch.Counter(epsilon=0.005, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [0, 1, 1, 0, 0, 1, 0, 1, 2, 0, 2, 1]:\n",
    "    cms.update(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [0, 1, 1, 0, 0, 1, 0, 1, 2, 0, 2, 1]:\n",
    "    counter[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(n=1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter().total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset('covtype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchstream.utils.visualization import visualize_results\n",
    "from pathlib import Path\n",
    "from batchstream.utils.reading_logs import load_drift_history, get_metrics_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(r'C:\\Users\\golik\\Desktop\\mgr\\praca\\first_draft\\out\\f3c094ec_arf_example_20230510_191733')\n",
    "log_dir = Path(((str)(out_dir)).replace(\"\\out\\\\\", \"\\log\\\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_name = log_dir.name\n",
    "path = os.path.join(log_dir, f\"{log_dir.name}_BatchPipeline.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def get_replacement_hist(out_dir):\n",
    "    log_dir = Path(((str)(out_dir)).replace(\"\\out\\\\\", \"\\log\\\\\"))\n",
    "    path = os.path.join(log_dir, f\"{log_dir.name}_BatchPipeline.log\")\n",
    "    replacement_hist = []\n",
    "    if not os.path.exists(path): return replacement_hist\n",
    "    with open(path, mode=\"r+\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if 'replacing' in line:\n",
    "                pattern = r'Iter=(\\d+)'\n",
    "                match = re.search(pattern, line)\n",
    "                if match:\n",
    "                    liczba = match.group(1)\n",
    "                    replacement_hist.append(int(liczba))\n",
    "\n",
    "    return replacement_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = out_dir\n",
    "print(str(d))\n",
    "drift_hist = load_drift_history(str(d))\n",
    "res = get_metrics_vals(str(d))\n",
    "visualize_results(res, drift_hist, 'rbf0.66', replacement_hist, metrics=['f1'], out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f for f in out_dir.resolve().glob('*') if f.is_file() and 'metadata' in f.name]\n",
    "with open(file_list[0], 'r') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_detectors_i = stream_pipeline['input_drift_detector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drift_detectors_metadata(drift_detectors):\n",
    "    if drift_detectors is None: return []\n",
    "    drift_handlers_metadata = []\n",
    "    for d_i in drift_detectors:\n",
    "        d_i_metadata = {}\n",
    "        d_i_monitor = d_i['monitor']['test_steps'][0]['target_drift_eval']\n",
    "        if d_i_monitor['type'] == 'EvidentlyMonitoringStep':\n",
    "            d_i_metadata.update({'min_instances': d_i_monitor['min_instances'],\n",
    "                                'name': d_i_monitor['name'],\n",
    "                                'clock': d_i_monitor['clock'],\n",
    "                                'min_instances': d_i_monitor['min_instances'],\n",
    "                                'type': 'Evidently'})\n",
    "        drift_handlers_metadata.append(d_i_metadata)\n",
    "    return drift_handlers_metadata\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_model_metadata(batch_model):\n",
    "    if batch_model['type'] == 'SklearnEstimator':\n",
    "        batch_alg = [x[0] for x in batch_model['sklearn_estimator']['steps'] if x[0] in ['rf']][0]\n",
    "    return batch_alg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_alg = [x[0] for x in batch_model['sklearn_estimator']['steps'] if x[0] in ['rf']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_pipeline = d['stream_pipeline']\n",
    "if stream_pipeline['type'] == 'BatchPipeline':\n",
    "    first_fit = stream_pipeline['min_samples_first_fit']\n",
    "    n_retrain = stream_pipeline['min_samples_retrain']\n",
    "    batch_model = stream_pipeline['batch_model']\n",
    "    batch_alg = get_batch_model_metadata(batch_model)\n",
    "    o_handlers = get_drift_detectors_metadata(stream_pipeline['input_drift_detector'])\n",
    "    i_handlers = get_drift_detectors_metadata(stream_pipeline['output_drift_detector'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = f'BatchPipeline ({batch_alg}) - n_min_retrain: {n_retrain}, n_first_fit: {first_fit}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_handlers_info_text(handlers, h_type):\n",
    "    info = ''\n",
    "    if len(handlers) >= 1:\n",
    "        info += f'{h_type} handlers:\\n'\n",
    "        for h in handlers:\n",
    "            info += f\"{h['type']} {h['name']} (clock: {h['clock']}, n_min: {h['min_instances']})\\n\"\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\Studia\\praca\\log\\20230509_1222_rbf66\\3f2a4d50_test_rf_data_evidently_ba6a_rbf66_20230509_004613\\3f2a4d50_test_rf_data_evidently_ba6a_rbf66_20230509_004613_BatchPipeline.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_combine_exp(suffix, n_curr=100, n_ref=100, n_online=10, window_size=100, n_first_fit=100,\n",
    "    data_stattest_threshold=0.05, target_stattest_threshold=0.05):\n",
    "\n",
    "\n",
    "    history = HistoryManager()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    ### Models comparison (after retraining)\n",
    "    #model_comparer = BatchModelComparer()\n",
    "    model_comparer = ShadowOnlineComparer(n_online=n_online)\n",
    "\n",
    "        ### Model composition\n",
    "    sklearn_batch_classifier = SklearnEstimator(Pipeline([('rf', RandomForestClassifier())]))\n",
    " \n",
    "    ###\n",
    "\n",
    "    \n",
    "    history2 = HistoryManager()\n",
    "    ### INPUT DRIFT DETECTION\n",
    "    # Detector 1.1 - Data Drift\n",
    "    data_drift_test_suite2 = {'tests': [\n",
    "        DataDriftTestPreset(stattest_threshold=data_stattest_threshold),\n",
    "    ]}\n",
    "    d12 = SimpleMonitoringStrategy(n_curr=n_curr, n_ref=n_ref)\n",
    "    ev12 = EvidentlyMonitoringStep(data_drift_test_suite2, d12, logger_factory, min_instances=2*n_curr, clock=n_curr, name='data_drift_eval')\n",
    "\n",
    "    # Detector 1.2 - Target Drift\n",
    "    target_drift2 = {'tests': [\n",
    "        TestColumnDrift(column_name='target', stattest_threshold=target_stattest_threshold),\n",
    "    ]}\n",
    "    d22 = SimpleMonitoringStrategy(n_curr=n_curr, n_ref=n_ref, type='target')\n",
    "    ev22 = EvidentlyMonitoringStep(target_drift2, d22, logger_factory, min_instances=2*n_curr, clock=n_curr, name='target_drift_eval')\n",
    "\n",
    "    input_monitoring2 = DriftMonitoringPipeline([(ev12.name, ev12), (ev22.name, ev22)])\n",
    "    input_drift_retraining_strategy2 = SimpleRetrainingStrategy(n_last_retrain=n_curr, n_last_test=0)\n",
    "    input_detector2 = DriftHandler(input_monitoring2, input_drift_retraining_strategy2)\n",
    "    ###\n",
    "\n",
    "\n",
    "    ### OUTPUT (PERFORMANCE) DRIFT DETECTION\n",
    "    # Detector 2.1 - Performance Drift\n",
    "\n",
    "    performance_drift2 = {'tests': [\n",
    "        TestPrecisionScore(),\n",
    "        TestRecallScore(),\n",
    "        TestF1Score(),\n",
    "        TestAccuracyScore()\n",
    "    ]}\n",
    "    d32 = SimpleMonitoringStrategy(n_curr=n_curr, n_ref=n_ref, type='prediction')\n",
    "    ev32 = EvidentlyMonitoringStep(performance_drift2, d32, logger_factory, min_instances=2*n_curr, clock=n_curr, name='performance_drift_eval')\n",
    "\n",
    "    output_monitoring2 = DriftMonitoringPipeline([(ev32.name, ev32)])\n",
    "    output_drift_retraining_strategy2 = SimpleRetrainingStrategy(n_last_retrain=n_curr, n_last_test=0)\n",
    "    output_detector2 = DriftHandler(output_monitoring2, output_drift_retraining_strategy2)\n",
    "    ###\n",
    "\n",
    "    ### Models comparison (after retraining)\n",
    "    #model_comparer = BatchModelComparer()\n",
    "    model_comparer2 = ShadowOnlineComparer(n_online=n_online)\n",
    "\n",
    "        ### Model composition\n",
    "    sklearn_batch_classifier2 = SklearnEstimator(Pipeline([('rf', RandomForestClassifier())]))\n",
    "    batch_pipeline2 = BatchPipeline(\n",
    "        sklearn_batch_classifier2,\n",
    "        input_drift_handlers=input_detector2,\n",
    "        output_drift_handlers=output_detector2,\n",
    "        history=history2,\n",
    "        logger_factory=logger_factory,\n",
    "        model_comparer=model_comparer2,\n",
    "        min_samples_retrain=n_curr,\n",
    "        min_samples_first_fit=n_first_fit\n",
    "    )\n",
    "    ###\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ###\n",
    "\n",
    "    arf_model = ARFClassifier(seed=42, leaf_prediction=\"mc\")\n",
    "    arf_pipe = RiverPipeline(arf_model)\n",
    "\n",
    "    ###\n",
    "    members = [batch_pipeline, batch_pipeline2, arf_pipe]\n",
    "    combiner = MajorityVoteCombiner()\n",
    "    comb_pipeline = CombinationPipeline(members=members, combiner=combiner)\n",
    "\n",
    "    ### Experiment args\n",
    "    experiment = StreamExperiment(comb_pipeline, eval_pipe, logger_factory)\n",
    "    \n",
    "    return experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f'_{datetime.today().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    \n",
    "logger_factory = LoggerFactory(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "str(uuid.uuid4())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"0c6b3787_rf_all_evidently_{str(uuid.uuid4())[:4]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets import synth\n",
    "\n",
    "\n",
    "dataset = synth.FriedmanDrift(\n",
    "    drift_type='gra',\n",
    "    position=(25_000, 50_000),\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sync_data.create_dataset import generate_friedman_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_friedman_drift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.read_data.get_dataset import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset('insects_abrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidently_input_handlers(n_curr, n_ref, data_stattest_threshold, target_stattest_threshold, logger_factory, data_drift=True, target_drift=True):\n",
    "    if not data_drift and not target_drift: return None\n",
    "    \n",
    "    ### INPUT DRIFT DETECTION\n",
    "    # Detector 1.1 - Data Drift\n",
    "    data_drift_test_suite = {'tests': [\n",
    "        DataDriftTestPreset(stattest_threshold=data_stattest_threshold),\n",
    "    ]}\n",
    "    d1 = SimpleMonitoringStrategy(n_curr=n_curr, n_ref=n_ref)\n",
    "    ev1 = EvidentlyMonitoringStep(data_drift_test_suite, d1, logger_factory, min_instances=2*n_curr, clock=n_curr, name='data_drift_eval')\n",
    "\n",
    "    # Detector 1.2 - Target Drift\n",
    "    target_drift = {'tests': [\n",
    "        TestColumnDrift(column_name='target', stattest_threshold=target_stattest_threshold),\n",
    "    ]}\n",
    "    d2 = SimpleMonitoringStrategy(n_curr=n_curr, n_ref=n_ref, type='target')\n",
    "    ev2 = EvidentlyMonitoringStep(target_drift, d2, logger_factory, min_instances=2*n_curr, clock=n_curr, name='target_drift_eval')\n",
    "\n",
    "    monitoring_steps = []\n",
    "    if data_drift:\n",
    "        monitoring_steps.append((ev1.name, ev1))\n",
    "    if target_drift:\n",
    "        monitoring_steps.append((ev2.name, ev2))\n",
    "    input_monitoring = DriftMonitoringPipeline(monitoring_steps)\n",
    "    \n",
    "    input_drift_retraining_strategy = SimpleRetrainingStrategy(n_last_retrain=n_curr, n_last_test=0)\n",
    "    input_detector = DriftHandler(input_monitoring, input_drift_retraining_strategy)\n",
    "\n",
    "    return input_detector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidently_output_handlers(n_curr, n_ref, logger_factory):\n",
    "    ### OUTPUT (PERFORMANCE) DRIFT DETECTION\n",
    "    # Detector 2.1 - Performance Drift\n",
    "\n",
    "    performance_drift = {'tests': [\n",
    "        TestPrecisionScore(),\n",
    "        TestRecallScore(),\n",
    "        TestF1Score(),\n",
    "        TestAccuracyScore()\n",
    "    ]}\n",
    "    d3 = SimpleMonitoringStrategy(n_curr=n_curr, n_ref=n_ref, type='prediction')\n",
    "    ev3 = EvidentlyMonitoringStep(performance_drift, d3, logger_factory, min_instances=2*n_curr, clock=n_curr, name='performance_drift_eval')\n",
    "\n",
    "    output_monitoring = DriftMonitoringPipeline([(ev3.name, ev3)])\n",
    "    output_drift_retraining_strategy = SimpleRetrainingStrategy(n_last_retrain=n_curr, n_last_test=0)\n",
    "    output_detector = DriftHandler(output_monitoring, output_drift_retraining_strategy)\n",
    "    \n",
    "    return output_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_pipeline(window_size):\n",
    "    eval_pipe = RiverEvaluationPipeline(metric_steps=[\n",
    "        (f'acc_preq_{window_size}', Rolling(Accuracy(), window_size)),\n",
    "        (f'macro_f1_preq_{window_size}', Rolling(MacroF1(), window_size)),\n",
    "        (f'kappa_preq_{window_size}', Rolling(CohenKappa(), window_size)),\n",
    "        ('acc', Accuracy()),\n",
    "        ('f1_macro', MacroF1()),\n",
    "        ('kappa', CohenKappa())\n",
    "    ])\n",
    "    return eval_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_pipeline(n_curr, n_first_fit, sklearn_pipe, input_handlers, output_handlers, model_comparer, logger_factory):\n",
    "    history = HistoryManager()\n",
    "    batch_pipeline = BatchPipeline(\n",
    "        sklearn_pipe,\n",
    "        input_drift_handlers=input_handlers,\n",
    "        output_drift_handlers=output_handlers,\n",
    "        history=history,\n",
    "        logger_factory=logger_factory,\n",
    "        model_comparer=model_comparer,\n",
    "        min_samples_retrain=n_curr,\n",
    "        min_samples_first_fit=n_first_fit\n",
    "    )\n",
    "    return batch_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment(suffix, n_curr=300, n_online=50, n_first_fit=300, window_size=100):\n",
    "    prefix = str(uuid.uuid4())[:8]\n",
    "    name = f'{prefix}_rf_all_evidently_{suffix}'\n",
    "    exp_name = f'{name}_{datetime.today().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    \n",
    "    logger_factory = LoggerFactory(exp_name)\n",
    "    input_handlers = get_evidently_input_handlers(n_curr=n_curr, n_ref=n_curr, data_stattest_threshold=0.05, target_stattest_threshold=0.05, logger_factory=logger_factory)\n",
    "    output_handlers = get_evidently_output_handlers(n_curr, n_ref=n_curr, logger_factory=logger_factory)\n",
    "    sklearn_pipeline = SklearnEstimator(Pipeline([('rf', RandomForestClassifier())]))\n",
    "    model_comparer = ShadowOnlineComparer(n_online=n_online)\n",
    "    batch_pipeline = get_batch_pipeline(n_curr, n_first_fit, sklearn_pipeline, input_handlers, output_handlers, model_comparer, logger_factory)\n",
    "    eval_pipe = get_eval_pipeline(window_size)\n",
    "    experiment = StreamExperiment(batch_pipeline, eval_pipe, logger_factory)\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.read_data.get_dataset import get_dataset\n",
    "\n",
    "exp = get_experiment('stagger2034')\n",
    "df = get_dataset('stagger_1k')\n",
    "exp.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets import synth\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.read_data.get_dataset import get_dataset\n",
    "df = get_dataset('stagger_1K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from batchstream.utils.visualization import visualize_results\n",
    "from batchstream.utils.reading_logs import load_drift_history, get_metrics_vals\n",
    "from utils.parse_metadata.parse_log_metadata import print_info\n",
    "import os \n",
    "\n",
    "\n",
    "d = Path(r'C:\\Users\\golik\\Desktop\\mgr\\praca\\first_draft\\out\\3abb3f56_rf_all_evidently_stagger1844_20230521_185417')\n",
    "\n",
    "print_info(d)\n",
    "replacement_hist = list(pd.read_csv(os.path.join(str(d), 'model_replacement_history.csv'), header=None)[0].values)\n",
    "drift_hist = load_drift_history(str(d))\n",
    "res = get_metrics_vals(str(d))\n",
    "if res is not None: \n",
    "    visualize_results(res, drift_hist, dataset_name='elec', replacement_hist=replacement_hist, metrics=['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from batchstream.utils.visualization import visualize_results\n",
    "from batchstream.utils.reading_logs import load_drift_history, get_metrics_vals\n",
    "from utils.parse_metadata.parse_log_metadata import print_info\n",
    "import os \n",
    "\n",
    "\n",
    "d = Path(r'C:\\Users\\golik\\Desktop\\mgr\\praca\\first_draft\\out\\c644433b_rf_all_evidently_stagger4_20230521_164748')\n",
    "\n",
    "print_info(d)\n",
    "replacement_hist = [] #list(pd.read_csv(os.path.join(str(d), 'model_replacement_history.csv'), header=None)[0].values)\n",
    "drift_hist = load_drift_history(str(d))\n",
    "res = get_metrics_vals(str(d))\n",
    "\n",
    "visualize_results(res, drift_hist, dataset_name='elec', replacement_hist=replacement_hist, metrics=['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c644433b_rf_all_evidently_stagger4_20230521_164748_data_drift_eval_ae5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['size', 'color', 'shape']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stagger_dataset(seed=42, balance_classes=True, drift_step: int=25_000, data_dir='./data'):\n",
    "    X = []\n",
    "    Y = []\n",
    "    dataset = synth.STAGGER(classification_function=0, seed=seed, balance_classes=balance_classes)\n",
    "    for x, y in dataset.take(drift_step):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    dataset.classification_function = 1\n",
    "    for x, y in dataset.take(drift_step):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    dataset.classification_function = 2\n",
    "    for x, y in dataset.take(drift_step):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    df = pd.DataFrame(X)\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "\n",
    "    df = pd.DataFrame(ohe.fit_transform(df).toarray())\n",
    "    df.columns = ['size_0', 'size_1', 'size_2', 'color_0', 'color_1', 'color_2', 'shape_0', 'shape_1', 'shape_2']\n",
    "\n",
    "    df['target'] = Y\n",
    "    \n",
    "    #df.to_csv(f'{data_dir}/STAGGER/stagger_{drift_step // 1000}K.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from os import path\n",
    "import json\n",
    "import os\n",
    "from river.datasets import synth\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_stagger_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sync_data.create_dataset import generate_stagger_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stagger_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stagger_dataset(drift_step=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sync_data.create_dataset import generate_LEDdrift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_LEDdrift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\golik\\.conda\\envs\\batchstream\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from experiments.second import get_adwin_experiment, get_evidently_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "rf = Pipeline([('rf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = get_evidently_experiment('xD', rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m exp2 \u001b[39m=\u001b[39m get_adwin_experiment(\u001b[39m'\u001b[39;49m\u001b[39mxxD\u001b[39;49m\u001b[39m'\u001b[39;49m, rf)\n",
      "File \u001b[1;32mc:\\Users\\golik\\Desktop\\mgr\\praca\\first_draft\\experiments\\second.py:172\u001b[0m, in \u001b[0;36mget_adwin_experiment\u001b[1;34m(suffix, sklearn_pipeline, n_online, n_first_fit, window_size, clock, grace_period, min_window_length, delta, adwin_detector_type, df_columns, logger_factory)\u001b[0m\n\u001b[0;32m    169\u001b[0m exp_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mtoday()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    171\u001b[0m logger_factory \u001b[39m=\u001b[39m LoggerFactory(exp_name)\n\u001b[1;32m--> 172\u001b[0m input_handlers \u001b[39m=\u001b[39m get_adwin_input_handlers(df_columns, clock, grace_period, min_window_length, delta, adwin_detector_type, logger_factory)\n\u001b[0;32m    173\u001b[0m output_handlers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    174\u001b[0m sklearn_pipeline \u001b[39m=\u001b[39m SklearnEstimator(sklearn_pipeline)\n",
      "File \u001b[1;32mc:\\Users\\golik\\Desktop\\mgr\\praca\\first_draft\\experiments\\second.py:100\u001b[0m, in \u001b[0;36mget_adwin_input_handlers\u001b[1;34m(df_columns, clock, grace_period, min_window_length, delta, adwin_detector_type, logger_factory)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m adwin_detector_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata_only\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m adwin_detector_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     99\u001b[0m     j \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 100\u001b[0m     \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m df_columns:\n\u001b[0;32m    101\u001b[0m         \u001b[39mif\u001b[39;00m col \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         \u001b[39mif\u001b[39;00m col \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "exp2 = get_adwin_experiment('xxD', rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
