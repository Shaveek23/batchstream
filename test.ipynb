{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja odpalana jako pandas_udf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class RetrainingStrategy(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_retraining_data(x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class EvaluationStrategyManager(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_curr_ref_data(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ModelEstimatorPipeline(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def handle(self, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_name(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "\n",
    "class ModelSklearnPipeline(ModelEstimatorPipeline):\n",
    "\n",
    "    def __init__(self, sklearn_pipeline: Pipeline, hyperparameter_space: Dict, retraining_strategy: TrainingStrategyManager):\n",
    "        self.estimator = sklearn_pipeline\n",
    "        self.hyperparameter_space = hyperparameter_space\n",
    "        self.retraining_strategy = retraining_strategy\n",
    "\n",
    "    def handle(self, x, y):\n",
    "        return self.estimator.predict(x)\n",
    "    \n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        x_train, y_train = self.retraining_strategy.get_retraining_data(x_history, y_history, prediction_history, drift_history)\n",
    "        self.estimator.fit(x_train, y_train) \n",
    "\n",
    "    def get_name(self):\n",
    "        return super().get_name() # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ModelRiverPipeline(ModelEstimatorPipeline):\n",
    "    \n",
    "    def __init__(self, river_pipeline: compose.Pipeline):\n",
    "        self.estimator = river_pipeline\n",
    "\n",
    "    def handle(self, x, y):\n",
    "        prediction = self.estimator.predict_one(x)\n",
    "        self.estimator.learn_one(x, y)\n",
    "        return prediction\n",
    "\n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        return\n",
    "    \n",
    "    def get_name(self):\n",
    "        return super().get_name() # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationPipeline:\n",
    "    \n",
    "    def __init__(self, metric_steps):\n",
    "        self.metric_steps = metric_steps\n",
    "\n",
    "    def handle(self, y_true, y_predict):\n",
    "        results = {}\n",
    "        for metric_name, metric in self.steps:\n",
    "            metric_value = metric.update(y_true, y_predict)\n",
    "            results.update({metric_name: metric_value})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class MonitoringStep(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def monitor(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]) -> bool:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.test_suite import TestSuite\n",
    "\n",
    "\n",
    "\n",
    "class EvidentlyMonitoringStep(MonitoringStep):\n",
    "\n",
    "    def __init__(self, evidently_test_suite: TestSuite, evaluation_strategy: EvaluationStrategyManager):\n",
    "        self.detector = evidently_test_suite\n",
    "        self.eval_strategy = evaluation_strategy\n",
    "\n",
    "    def monitor(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]) -> bool:\n",
    "        curr, ref = self.eval_strategy.get_curr_ref_data(x_history, y_history, prediction_history, drift_history)\n",
    "        self.detector.run(reference_data=ref,current_data=curr)\n",
    "        report = self.detector.as_dict()\n",
    "        return True # to do based on report\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment -> uruchomiony na danym partition: definiuje experiment pipeline\n",
    "\n",
    "\n",
    "\n",
    "datastream_name = datastream['name'][0]\n",
    "datastream = datastream.drop(col=['name'])\n",
    "logger = Logger(dataset_name=name)\n",
    "\n",
    "logger.start()\n",
    "for x, y in datastream:\n",
    "\n",
    "   logger.iter()\n",
    "   pipe = StreamClassificationPipeline()\n",
    "\n",
    "   pipe.handle(x, y)\n",
    "   logger.iter_end()\n",
    "\n",
    "logger.end()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list([{'a': 4}][0].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.drift import ADWIN\n",
    "\n",
    "a = ADWIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'_helper'.startswith('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(a).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{item for item in vars(a).items() if not item[0].startswith('_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, ensemble, model_selection\n",
    "\n",
    "from evidently import ColumnMapping\n",
    "from evidently.test_suite import TestSuite\n",
    "\n",
    "from evidently.test_preset import NoTargetPerformanceTestPreset\n",
    "from evidently.test_preset import DataQualityTestPreset\n",
    "from evidently.test_preset import DataStabilityTestPreset\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.test_preset import RegressionTestPreset\n",
    "from evidently.test_preset import MulticlassClassificationTestPreset\n",
    "from evidently.test_preset import BinaryClassificationTopKTestPreset\n",
    "from evidently.test_preset import BinaryClassificationTestPreset\n",
    "\n",
    "from evidently.tests import TestNumberOfEmptyRows, TestNumberOfEmptyColumns, TestNumberOfDuplicatedRows, TestNumberOfDuplicatedColumns, TestNumberOfDriftedColumns, TestShareOfDriftedColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for Data Quality and Integrity\n",
    "adult_data = datasets.fetch_openml(name='adult', version=2, as_frame='auto')\n",
    "adult = adult_data.frame\n",
    "\n",
    "adult_ref = adult[~adult.education.isin(['Some-college', 'HS-grad', 'Bachelors'])]\n",
    "adult_cur = adult[adult.education.isin(['Some-college', 'HS-grad', 'Bachelors'])]\n",
    "\n",
    "adult_cur.iloc[:2000, 3:5] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for Regression\n",
    "housing_data = datasets.fetch_california_housing(as_frame='auto')\n",
    "housing = housing_data.frame\n",
    "\n",
    "housing.rename(columns={'MedHouseVal': 'target'}, inplace=True)\n",
    "housing['prediction'] = housing_data['target'].values + np.random.normal(0, 3, housing.shape[0])\n",
    "\n",
    "housing_ref = housing.sample(n=5000, replace=False)\n",
    "housing_cur = housing.sample(n=5000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for Binary Probabilistic Classifcation\n",
    "bcancer_data = datasets.load_breast_cancer(as_frame='auto')\n",
    "bcancer = bcancer_data.frame\n",
    "\n",
    "bcancer_ref = bcancer.sample(n=300, replace=False)\n",
    "bcancer_cur = bcancer.sample(n=200, replace=False)\n",
    "\n",
    "bcancer_label_ref = bcancer_ref.copy(deep=True)\n",
    "bcancer_label_cur = bcancer_cur.copy(deep=True)\n",
    "\n",
    "model = ensemble.RandomForestClassifier(random_state=1, n_estimators=10)\n",
    "model.fit(bcancer_ref[bcancer_data.feature_names.tolist()], bcancer_ref.target)\n",
    "\n",
    "bcancer_ref['prediction'] = model.predict_proba(bcancer_ref[bcancer_data.feature_names.tolist()])[:, 1]\n",
    "bcancer_cur['prediction'] = model.predict_proba(bcancer_cur[bcancer_data.feature_names.tolist()])[:, 1]\n",
    "\n",
    "bcancer_label_ref['prediction'] = model.predict(bcancer_label_ref[bcancer_data.feature_names.tolist()])\n",
    "bcancer_label_cur['prediction'] = model.predict(bcancer_label_cur[bcancer_data.feature_names.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for Multiclass Classifcation\n",
    "iris_data = datasets.load_iris(as_frame='auto')\n",
    "iris = iris_data.frame\n",
    "\n",
    "iris_ref = iris.sample(n=75, replace=False)\n",
    "iris_cur = iris.sample(n=75, replace=False)\n",
    "\n",
    "model = ensemble.RandomForestClassifier(random_state=1, n_estimators=3)\n",
    "model.fit(iris_ref[iris_data.feature_names], iris_ref.target)\n",
    "\n",
    "iris_ref['prediction'] = model.predict(iris_ref[iris_data.feature_names])\n",
    "iris_cur['prediction'] = model.predict(iris_cur[iris_data.feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drift_dataset_tests = TestSuite(tests=[\n",
    "    TestNumberOfEmptyRows(),\n",
    "    TestNumberOfEmptyColumns(),\n",
    "    TestNumberOfDuplicatedRows(),\n",
    "    TestNumberOfDuplicatedColumns(),\n",
    "    TestNumberOfDriftedColumns(),\n",
    "    TestShareOfDriftedColumns(),\n",
    "])\n",
    "\n",
    "data_drift_dataset_tests.run(reference_data=adult_ref, current_data=adult_cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test preset as a python object\n",
    "res = data_drift_dataset_tests.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['tests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_drift_detected = False\n",
    "detection_idx = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "x = Counter({'SUCCESS': 3, 'FAIL': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    return 1, 2, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, kwargs = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo:\n",
    "\n",
    "    def __init__(self, a: int, b: int, weight: int=4, height: int=3):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.weight = weight\n",
    "        self.height = height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= Foo(a, b, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "            'tests': [{\n",
    "                'name': 'self.step_name',\n",
    "                'description': f'Drift detected at idx.: {3}',\n",
    "                'status': 'FAIL',\n",
    "                'group': 'river-detector',\n",
    "                'parameters': {\n",
    "                    'detected_at_idx': 3,\n",
    "                    'detector_type': 'type(self.detector)'\n",
    "                }.update({'item for item in vars(self.detector).items()' : \"if not item[0].startswith('_')\"})\n",
    "            }],\n",
    "            'summary': {\n",
    "                'all_passed': 0,\n",
    "                'total_tests': 1,\n",
    "                'success_tests': 0,\n",
    "                'failed_tests': 1,\n",
    "                'by_status': Counter({'SUCCESS': 0, 'FAIL': 1})}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_failed = d['summary']['failed_tests'] > 0\n",
    "all_failed = d['summary']['failed_tests'] == d['summary']['total_tests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.random(size=(10, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_curr = 5\n",
    "n_ref = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-n_curr:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-(n_curr + n_ref):-n_curr,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/ADS/internet_ads.arff.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.read_data.internet_ads import get_internet_ads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_internet_ads_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchstream.pipelines.stream.model_river_pipeline import ModelRiverPipeline\n",
    "from batchstream.evaluation.model_evaluation_pipeline import ModelEvaluationPipeline\n",
    "from river.metrics import Accuracy, ROCAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "from river import preprocessing\n",
    "from batchstream.utils.logging.performance_logger import PerformanceEvalLogger\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "river_model = scaler | log_reg\n",
    "perf_logger = PerformanceEvalLogger('test-xgrhtueifj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_pipeline = ModelRiverPipeline(river_model)\n",
    "\n",
    "acc = Accuracy()\n",
    "roc_auc = ROCAUC()\n",
    "pipeline_evaluation = ModelEvaluationPipeline(metric_steps=[('roc_auc', roc_auc)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchstream.experiment.experiment import StreamExperiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = StreamExperiment(stream_pipeline, pipeline_evaluation, perf_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_batch = [\n",
    "        {'accuracy': 0.0, 'roc_auc': -0.0},\n",
    "        {'accuracy': 0.5, 'roc_auc': 0.5},\n",
    "        {'accuracy': 0.33, 'roc_auc': 0.5},\n",
    "        {'accuracy': 0.5, 'roc_auc': 0.5},\n",
    "        {'accuracy': 0.6, 'roc_auc': 0.67},\n",
    "        {'accuracy': 0.67, 'roc_auc': 0.67},\n",
    "        {'accuracy': 0.71, 'roc_auc': 0.75},\n",
    "        {'accuracy': 0.75, 'roc_auc': 0.75}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_logger.log_eval_report(report_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_batch_2 = [\n",
    "        {'accuracy': 0.75, 'roc_auc': -0.0},\n",
    "        {'accuracy': 0.71, 'roc_auc': 0.5},\n",
    "        {'accuracy': 0.67, 'roc_auc': 0.5},\n",
    "        {'accuracy': 0.6, 'roc_auc': 0.5},\n",
    "        {'accuracy': 0.5, 'roc_auc': 0.67},\n",
    "        {'accuracy': 0.4, 'roc_auc': 0.67},\n",
    "        {'accuracy': 0.33, 'roc_auc': 0.75},\n",
    "        {'accuracy': 0.2, 'roc_auc': 0.75}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_logger.log_eval_report(report_batch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_logger.log_info(\"Start evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_logger.log_info(\"First batch logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_logger.log_info(\"Second batch logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_logger.log_info(\"End evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_artifact = pd.read_csv('./out/1111/1111_performance_eval_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./log/1111/1111_performance_eval.log\", 'r') as fp:\n",
    "    for count, line in enumerate(fp):\n",
    "        pass\n",
    "return count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(f'./log/test_experiment')\n",
    "shutil.rmtree(f'./out/test_experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
