{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja odpalana jako pandas_udf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class RetrainingStrategy(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_retraining_data(x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class EvaluationStrategyManager(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_curr_ref_data(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ModelEstimatorPipeline(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def handle(self, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_name(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "\n",
    "class ModelSklearnPipeline(ModelEstimatorPipeline):\n",
    "\n",
    "    def __init__(self, sklearn_pipeline: Pipeline, hyperparameter_space: Dict, retraining_strategy: TrainingStrategyManager):\n",
    "        self.estimator = sklearn_pipeline\n",
    "        self.hyperparameter_space = hyperparameter_space\n",
    "        self.retraining_strategy = retraining_strategy\n",
    "\n",
    "    def handle(self, x, y):\n",
    "        return self.estimator.predict(x)\n",
    "    \n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        x_train, y_train = self.retraining_strategy.get_retraining_data(x_history, y_history, prediction_history, drift_history)\n",
    "        self.estimator.fit(x_train, y_train) \n",
    "\n",
    "    def get_name(self):\n",
    "        return super().get_name() # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ModelRiverPipeline(ModelEstimatorPipeline):\n",
    "    \n",
    "    def __init__(self, river_pipeline: compose.Pipeline):\n",
    "        self.estimator = river_pipeline\n",
    "\n",
    "    def handle(self, x, y):\n",
    "        prediction = self.estimator.predict_one(x)\n",
    "        self.estimator.learn_one(x, y)\n",
    "        return prediction\n",
    "\n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        return\n",
    "    \n",
    "    def get_name(self):\n",
    "        return super().get_name() # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationPipeline:\n",
    "    \n",
    "    def __init__(self, metric_steps):\n",
    "        self.metric_steps = metric_steps\n",
    "\n",
    "    def handle(self, y_true, y_predict):\n",
    "        results = {}\n",
    "        for metric_name, metric in self.steps:\n",
    "            metric_value = metric.update(y_true, y_predict)\n",
    "            results.update({metric_name: metric_value})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class MonitoringStep(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def monitor(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]) -> bool:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.test_suite import TestSuite\n",
    "\n",
    "\n",
    "\n",
    "class EvidentlyMonitoringStep(MonitoringStep):\n",
    "\n",
    "    def __init__(self, evidently_test_suite: TestSuite, evaluation_strategy: EvaluationStrategyManager):\n",
    "        self.detector = evidently_test_suite\n",
    "        self.eval_strategy = evaluation_strategy\n",
    "\n",
    "    def monitor(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]) -> bool:\n",
    "        curr, ref = self.eval_strategy.get_curr_ref_data(x_history, y_history, prediction_history, drift_history)\n",
    "        self.detector.run(reference_data=ref,current_data=curr)\n",
    "        report = self.detector.as_dict()\n",
    "        return True # to do based on report\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.tests import *\n",
    "from batchstream.history.base.history_manager import HistoryManager\n",
    "from batchstream.monitoring.pipeline.model_monitoring_pipeline import ModelMonitoringPipeline\n",
    "from batchstream.monitoring.pipeline.steps.batch.evidently_monitoring_step import EvidentlyMonitoringStep\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "from batchstream.batch_monitoring_strategy.dummy_monitoring_strategy import DummyMonitoringStrategy\n",
    "from batchstream.retraining_strategy.dummy_retraining_strategy import DummyRetrainingStrategy \n",
    "from batchstream.model_comparers.batch_comparer import BatchModelComparer\n",
    "from batchstream.model_comparers.shadow_comparer import ShadowOnlineComparer\n",
    "from batchstream.pipelines.batch.batch_pipeline import BatchPipeline\n",
    "from batchstream.estimators.sklearn_estimator import SklearnEstimator\n",
    "from batchstream.detectors.base.detector import DriftDetector\n",
    "from batchstream.experiment.experiment import StreamExperiment\n",
    "from river.metrics import Accuracy, ROCAUC\n",
    "from batchstream.evaluation.model_evaluation_pipeline import ModelEvaluationPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "history = HistoryManager()\n",
    "logger_factory = LoggerFactory('test-2216')\n",
    "\n",
    "\n",
    "### INPUT DRIFT DETECTION\n",
    "# Detector 1.1 - Data Drift\n",
    "data_drift_test_suite = TestSuite(tests=[\n",
    "   DataDriftTestPreset(),\n",
    "])\n",
    "d1 = DummyMonitoringStrategy(n_curr=120, n_ref=120)\n",
    "ev1 = EvidentlyMonitoringStep(data_drift_test_suite, d1, logger_factory, min_instances=240, clock=120, name='data_drift_eval')\n",
    "\n",
    "# Detector 1.2 - Target Drift\n",
    "target_drift = TestSuite(tests=[\n",
    "    TestColumnDrift(column_name='target'),\n",
    "])\n",
    "d2 = DummyMonitoringStrategy(n_curr=120, n_ref=120, type='target')\n",
    "ev2 = EvidentlyMonitoringStep(target_drift, d2, logger_factory, min_instances=240, clock=120, name='target_drift_eval')\n",
    "\n",
    "input_monitoring = ModelMonitoringPipeline([(ev1._name, ev1), (ev2._name, ev2)])\n",
    "input_drift_retraining_strategy = DummyRetrainingStrategy(n_last_retrain=120, n_last_test=0)\n",
    "input_detector = DriftDetector(input_monitoring, input_drift_retraining_strategy)\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OUTPUT (PERFORMANCE) DRIFT DETECTION\n",
    "# Detector 2.1 - Performance Drift\n",
    "\n",
    "performance_drift = TestSuite(tests=[\n",
    "    TestPrecisionScore(),\n",
    "    TestRecallScore(),\n",
    "    TestF1Score(),\n",
    "    TestAccuracyScore()\n",
    "])\n",
    "d3 = DummyMonitoringStrategy(n_curr=120, n_ref=120, type='prediction')\n",
    "ev3 = EvidentlyMonitoringStep(performance_drift, d3, logger_factory, min_instances=360, clock=120, name='performance_drift_eval')\n",
    "\n",
    "output_monitoring = ModelMonitoringPipeline([(ev3._name, ev3)])\n",
    "output_drift_retraining_strategy = DummyRetrainingStrategy(n_last_retrain=120, n_last_test=0)\n",
    "output_detector = DriftDetector(output_monitoring, output_drift_retraining_strategy)\n",
    "###\n",
    "\n",
    "### Models comparison (after retraining)\n",
    "#model_comparer = BatchModelComparer()\n",
    "model_comparer = ShadowOnlineComparer(n_online=20)\n",
    "###\n",
    "\n",
    "\n",
    "### Model's Performance Evaluation\n",
    "acc = Accuracy()\n",
    "roc_auc = ROCAUC()\n",
    "eval_pipe = ModelEvaluationPipeline(metric_steps=[\n",
    "    ('accuracy', acc),\n",
    "    ('roc_auc', roc_auc)\n",
    "])\n",
    "###\n",
    "\n",
    "\n",
    "### Model composition\n",
    "sklearn_batch_classifier = SklearnEstimator(Pipeline([('rf', RandomForestClassifier())]))\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_detector=input_detector,\n",
    "    output_drift_detector=output_detector,\n",
    "    history=history,\n",
    "    logger_factory=logger_factory,\n",
    "    model_comparer=model_comparer,\n",
    "    min_samples_retrain=120,\n",
    "    min_samples_first_fit=240\n",
    ")\n",
    "###\n",
    "\n",
    "### Experiment\n",
    "experiment = StreamExperiment(batch_pipeline, eval_pipe, logger_factory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_breast_cancer(return_X_y=True)\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:test-2216_data_drift_eval:Drift detected at: 360.\n",
      "INFO:test-2216_BatchPipeline:Iter=360: drift detected. in detector num.: 0\n",
      "INFO:test-2216_BatchPipeline:Iter=360: Comparing the old and the retrained model.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "experiment.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_drift_retraining_strategy.get_retraining_data(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
