{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja odpalana jako pandas_udf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class RetrainingStrategy(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_retraining_data(x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class EvaluationStrategyManager(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_curr_ref_data(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ModelEstimatorPipeline(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def handle(self, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_name(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "\n",
    "class ModelSklearnPipeline(ModelEstimatorPipeline):\n",
    "\n",
    "    def __init__(self, sklearn_pipeline: Pipeline, hyperparameter_space: Dict, retraining_strategy: TrainingStrategyManager):\n",
    "        self.estimator = sklearn_pipeline\n",
    "        self.hyperparameter_space = hyperparameter_space\n",
    "        self.retraining_strategy = retraining_strategy\n",
    "\n",
    "    def handle(self, x, y):\n",
    "        return self.estimator.predict(x)\n",
    "    \n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        x_train, y_train = self.retraining_strategy.get_retraining_data(x_history, y_history, prediction_history, drift_history)\n",
    "        self.estimator.fit(x_train, y_train) \n",
    "\n",
    "    def get_name(self):\n",
    "        return super().get_name() # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ModelRiverPipeline(ModelEstimatorPipeline):\n",
    "    \n",
    "    def __init__(self, river_pipeline: compose.Pipeline):\n",
    "        self.estimator = river_pipeline\n",
    "\n",
    "    def handle(self, x, y):\n",
    "        prediction = self.estimator.predict_one(x)\n",
    "        self.estimator.learn_one(x, y)\n",
    "        return prediction\n",
    "\n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        return\n",
    "    \n",
    "    def get_name(self):\n",
    "        return super().get_name() # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationPipeline:\n",
    "    \n",
    "    def __init__(self, metric_steps):\n",
    "        self.metric_steps = metric_steps\n",
    "\n",
    "    def handle(self, y_true, y_predict):\n",
    "        results = {}\n",
    "        for metric_name, metric in self.steps:\n",
    "            metric_value = metric.update(y_true, y_predict)\n",
    "            results.update({metric_name: metric_value})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class MonitoringStep(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def monitor(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]) -> bool:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.test_suite import TestSuite\n",
    "\n",
    "\n",
    "\n",
    "class EvidentlyMonitoringStep(MonitoringStep):\n",
    "\n",
    "    def __init__(self, evidently_test_suite: TestSuite, evaluation_strategy: EvaluationStrategyManager):\n",
    "        self.detector = evidently_test_suite\n",
    "        self.eval_strategy = evaluation_strategy\n",
    "\n",
    "    def monitor(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]) -> bool:\n",
    "        curr, ref = self.eval_strategy.get_curr_ref_data(x_history, y_history, prediction_history, drift_history)\n",
    "        self.detector.run(reference_data=ref,current_data=curr)\n",
    "        report = self.detector.as_dict()\n",
    "        return True # to do based on report\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detector\n",
    "\n",
    "    ## Monitoring - when?\n",
    "        #[Adwin]\n",
    "        #[EvidentlyAI TestSuite]\n",
    "\n",
    "\n",
    "\n",
    "    ## Retraining Strategy - which retraining data?\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'1': [0, 1, 2], '2': [9, 8, 7]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.read_data.covtype import get_covtype_dataset\n",
    "\n",
    "\n",
    "\n",
    "df = get_covtype_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1].loc['Elevation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rng = random.Random(12345)\n",
    "\n",
    "data_stream = rng.choices([0, 1], k=1000) + rng.choices(range(4, 8), k=1000)\n",
    "\n",
    "# Update drift detector and verify if change is detected\n",
    "for i, val in enumerate(data_stream):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.tests import *\n",
    "from batchstream.history.base.history_manager import HistoryManager\n",
    "from batchstream.monitoring.pipeline.model_monitoring_pipeline import ModelMonitoringPipeline\n",
    "from batchstream.monitoring.pipeline.steps.batch.evidently_monitoring_step import EvidentlyMonitoringStep\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "from batchstream.batch_monitoring_strategy.dummy_monitoring_strategy import DummyMonitoringStrategy\n",
    "from batchstream.retraining_strategy.dummy_retraining_strategy import DummyRetrainingStrategy \n",
    "from batchstream.model_comparers.batch_comparer import BatchModelComparer\n",
    "from batchstream.model_comparers.shadow_comparer import ShadowOnlineComparer\n",
    "from batchstream.pipelines.batch.batch_pipeline import BatchPipeline\n",
    "from batchstream.estimators.sklearn_estimator import SklearnEstimator\n",
    "from batchstream.detectors.base.detector import DriftDetector\n",
    "from batchstream.experiment.experiment import StreamExperiment\n",
    "from river.metrics import Accuracy, ROCAUC\n",
    "from batchstream.evaluation.model_evaluation_pipeline import ModelEvaluationPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "history = HistoryManager()\n",
    "logger_factory = LoggerFactory('test-2220')\n",
    "\n",
    "\n",
    "### INPUT DRIFT DETECTION\n",
    "# Detector 1.1 - Data Drift\n",
    "data_drift_test_suite = TestSuite(tests=[\n",
    "   DataDriftTestPreset(),\n",
    "])\n",
    "d1 = DummyMonitoringStrategy(n_curr=120, n_ref=120)\n",
    "ev1 = EvidentlyMonitoringStep(data_drift_test_suite, d1, logger_factory, min_instances=240, clock=120, name='data_drift_eval')\n",
    "\n",
    "# Detector 1.2 - Target Drift\n",
    "target_drift = TestSuite(tests=[\n",
    "    TestColumnDrift(column_name='target'),\n",
    "])\n",
    "d2 = DummyMonitoringStrategy(n_curr=120, n_ref=120, type='target')\n",
    "ev2 = EvidentlyMonitoringStep(target_drift, d2, logger_factory, min_instances=240, clock=120, name='target_drift_eval')\n",
    "\n",
    "input_monitoring = ModelMonitoringPipeline([(ev1._name, ev1), (ev2._name, ev2)])\n",
    "input_drift_retraining_strategy = DummyRetrainingStrategy(n_last_retrain=120, n_last_test=0)\n",
    "input_detector = DriftDetector(input_monitoring, input_drift_retraining_strategy)\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OUTPUT (PERFORMANCE) DRIFT DETECTION\n",
    "# Detector 2.1 - Performance Drift\n",
    "\n",
    "performance_drift = TestSuite(tests=[\n",
    "    TestPrecisionScore(),\n",
    "    TestRecallScore(),\n",
    "    TestF1Score(),\n",
    "    TestAccuracyScore()\n",
    "])\n",
    "d3 = DummyMonitoringStrategy(n_curr=120, n_ref=120, type='prediction')\n",
    "ev3 = EvidentlyMonitoringStep(performance_drift, d3, logger_factory, min_instances=360, clock=120, name='performance_drift_eval')\n",
    "\n",
    "output_monitoring = ModelMonitoringPipeline([(ev3._name, ev3)])\n",
    "output_drift_retraining_strategy = DummyRetrainingStrategy(n_last_retrain=120, n_last_test=0)\n",
    "output_detector = DriftDetector(output_monitoring, output_drift_retraining_strategy)\n",
    "###\n",
    "\n",
    "### Models comparison (after retraining)\n",
    "#model_comparer = BatchModelComparer()\n",
    "model_comparer = ShadowOnlineComparer(n_online=20)\n",
    "###\n",
    "\n",
    "\n",
    "### Model's Performance Evaluation\n",
    "acc = Accuracy()\n",
    "roc_auc = ROCAUC()\n",
    "eval_pipe = ModelEvaluationPipeline(metric_steps=[\n",
    "    ('accuracy', acc),\n",
    "    ('roc_auc', roc_auc)\n",
    "])\n",
    "###\n",
    "\n",
    "\n",
    "### Model composition\n",
    "sklearn_batch_classifier = SklearnEstimator(Pipeline([('rf', RandomForestClassifier())]))\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_detector=input_detector,\n",
    "    output_drift_detector=output_detector,\n",
    "    history=history,\n",
    "    logger_factory=logger_factory,\n",
    "    model_comparer=model_comparer,\n",
    "    min_samples_retrain=120,\n",
    "    min_samples_first_fit=240\n",
    ")\n",
    "###\n",
    "\n",
    "### Experiment\n",
    "experiment = StreamExperiment(batch_pipeline, eval_pipe, logger_factory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_breast_cancer(return_X_y=True)\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.metrics import Accuracy\n",
    "from river import utils\n",
    "\n",
    "Y_true =    [1, 0, 1, 0, 1, 0, 0]\n",
    "Y_predict = [1, 1, 0, 0, 1, 1, 1]\n",
    "\n",
    "\n",
    "roll_acc = utils.Rolling(Accuracy(), window_size=3)\n",
    "for y_t, y_p in zip(Y_true, Y_predict):\n",
    "    print(roll_acc.update(y_t, y_p).get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "\n",
    "\n",
    "\n",
    "data_drift_test_suite = TestSuite(tests=[\n",
    "    DataDriftTestPreset(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drift_test_suite._inner_suite.context.tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.tests.base_test import Test\n",
    "\n",
    "t: Test = data_drift_test_suite._inner_suite.context.tests[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchstream.pipelines.base.stream_pipeline import StreamPipeline\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "\n",
    "\n",
    "class CombinationPipeline(StreamPipeline):\n",
    "\n",
    "    def __init__(self, members: List[StreamPipeline], combiner: object):\n",
    "        self._members = members\n",
    "        self._combiner = combiner\n",
    "\n",
    "    def handle(self, x, y) -> int:\n",
    "        return self._combiner.combine(x, y, self._members)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.tests import *\n",
    "from batchstream.history.base.history_manager import HistoryManager\n",
    "from batchstream.monitoring.pipeline.model_monitoring_pipeline import ModelMonitoringPipeline\n",
    "from batchstream.monitoring.pipeline.steps.batch.evidently_monitoring_step import EvidentlyMonitoringStep\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "from batchstream.batch_monitoring_strategy.simple_monitoring_strategy import SimpleMonitoringStrategy\n",
    "from batchstream.retraining_strategy.simple_retraining_strategy import SimpleRetrainingStrategy \n",
    "from batchstream.model_comparers.batch_comparer import BatchModelComparer\n",
    "from batchstream.model_comparers.shadow_comparer import ShadowOnlineComparer\n",
    "from batchstream.pipelines.batch.batch_pipeline import BatchPipeline\n",
    "from batchstream.estimators.sklearn_estimator import SklearnEstimator\n",
    "from batchstream.detectors.base.drift_handler import DriftHandler\n",
    "from batchstream.experiment.experiment import StreamExperiment\n",
    "from river.metrics import Accuracy, ROCAUC\n",
    "from batchstream.evaluation.river_evaluation_pipeline import RiverEvaluationPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory('test-2218')\n",
    "retraining_strategy = SimpleRetrainingStrategy(\n",
    "  n_last_retrain=500, n_last_test=0)\n",
    "\n",
    "\n",
    "### INPUT DRIFT DETECTION\n",
    "# Detector 1.1 - Data Drift\n",
    "data_drift_test_suite = TestSuite(tests=[\n",
    "DataDriftTestPreset(),\n",
    "])\n",
    "d1 = SimpleMonitoringStrategy(n_curr=120, n_ref=120)\n",
    "ev1 = EvidentlyMonitoringStep(data_drift_test_suite, d1, logger_factory, min_instances=240, clock=120, name='data_drift_eval')\n",
    "\n",
    "# Detector 1.2 - Target Drift\n",
    "target_drift = TestSuite(tests=[\n",
    "    TestColumnDrift(column_name='target'),\n",
    "])\n",
    "d2 = SimpleMonitoringStrategy(n_curr=120, n_ref=120, type='target')\n",
    "ev2 = EvidentlyMonitoringStep(target_drift, d2, logger_factory, min_instances=240, clock=120, name='target_drift_eval')\n",
    "\n",
    "input_monitoring = ModelMonitoringPipeline([(ev1._name, ev1), (ev2._name, ev2)])\n",
    "input_detector = DriftHandler(input_monitoring, retraining_strategy)\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OUTPUT (PERFORMANCE) DRIFT DETECTION\n",
    "# Detector 2.1 - Performance Drift\n",
    "\n",
    "performance_drift = TestSuite(tests=[\n",
    "    TestPrecisionScore(),\n",
    "    TestRecallScore(),\n",
    "    TestF1Score(),\n",
    "    TestAccuracyScore()\n",
    "])\n",
    "d3 = SimpleMonitoringStrategy(n_curr=500, n_ref=500, type='prediction')\n",
    "ev3 = EvidentlyMonitoringStep(performance_drift, d3, logger_factory,\n",
    "  min_instances=1000, clock=500, name='performance_drift_eval')\n",
    "\n",
    "# Output Drift Handler (Performance Drift + Retraining Strategy)\n",
    "output_monitoring = ModelMonitoringPipeline([(ev3._name, ev3)])\n",
    "\n",
    "output_drift_handlers = [\n",
    "  DriftHandler(output_monitoring, retraining_strategy)\n",
    "]\n",
    "###\n",
    "\n",
    "### Models comparison (after retraining)\n",
    "#model_comparer = BatchModelComparer()\n",
    "model_comparer = ShadowOnlineComparer(n_online=100)\n",
    "###\n",
    "\n",
    "\n",
    "### Model's Performance Evaluation\n",
    "acc = Accuracy()\n",
    "roc_auc = ROCAUC()\n",
    "eval_pipe = RiverEvaluationPipeline(metric_steps=[\n",
    "    ('accuracy', acc),\n",
    "    ('roc_auc', roc_auc)\n",
    "])\n",
    "###\n",
    "\n",
    "\n",
    "### Model composition\n",
    "logger_factory = LoggerFactory(experiment_id='rf_exp')\n",
    "Pipeline([('rf', RandomForestClassifier(max_depth=10))])\n",
    "sklearn_batch_classifier = SklearnEstimator()\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.metrics import Accuracy, MacroF1, MicroF1\n",
    "from river.utils import Rolling\n",
    "\n",
    "\n",
    "\n",
    "window_size = 1000\n",
    "eval_pipe = RiverEvaluationPipeline(metric_steps=[\n",
    "  (f'acc_preq_{window_size}', Rolling(Accuracy(), window_size)),\n",
    "  (f'micro_f1_preq_{window_size}', Rolling(MicroF1(), window_size)),\n",
    "  (f'macro_f1_preq_{window_size}', Rolling(MacroF1(), window_size))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchstream.pipelines.stream.model_river_pipeline import RiverPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.forest import ARFClassifier\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river.datasets import synth\n",
    "from river import tree\n",
    "from river import ADWIN\n",
    "from river import naive_bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.ensemble import SRPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.forest import ARFClassifier\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='arf_exp')\n",
    "arf_model = ARFClassifier(seed=42, leaf_prediction=\"mc\")\n",
    "arf_pipe = RiverPipeline(arf_model)\n",
    "arf_experiment = StreamExperiment(arf_pipe, eval_pipe, logger_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tree.HoeffdingTreeClassifier(grace_period=50, delta=0.01, nominal_attributes=['age', 'car', 'zipcode'])\n",
    "srp_model = ensemble.SRPClassifier(model=base_model, n_models=3, seed=42)\n",
    "srp_pipe = RiverPipeline(srp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree import HoeffdingAdaptiveTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "hat_model = HoeffdingAdaptiveTreeClassifier(\n",
    "  grace_period=100,\n",
    "  delta=1e-5,\n",
    "  leaf_prediction='nb',\n",
    "  nb_threshold=10,\n",
    "  seed=42\n",
    ")\n",
    "hat_pipe = RiverPipeline(hat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import naive_bayes\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='nb_exp')\n",
    "nb_model =  naive_bayes.GaussianNB()\n",
    "nb_pipe = RiverPipeline(nb_model)\n",
    "nb_experiment = StreamExperiment(nb_pipe, eval_pipe, logger_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.tests import *\n",
    "\n",
    "\n",
    "\n",
    "### INPUT DRIFT DETECTION\n",
    "# Detector 1.1 - Data Drift\n",
    "data_drift_test_suite = TestSuite(tests=[\n",
    "  DataDriftTestPreset(),\n",
    "])\n",
    "d1 = DummyMonitoringStrategy(n_curr=500, n_ref=500, type='data')\n",
    "ev1 = EvidentlyMonitoringStep(data_drift_test_suite, d1, logger_factory,\n",
    "  min_instances=1000, clock=500, name='data_drift_eval'\n",
    ")\n",
    "\n",
    "# Detector 1.2 - Target Drift \n",
    "target_drift = TestSuite(tests=[\n",
    "    TestColumnDrift(column_name='target'),\n",
    "])\n",
    "d2 = DummyMonitoringStrategy(n_curr=500, n_ref=500, type='target')\n",
    "ev2 = EvidentlyMonitoringStep(target_drift, d2, logger_factory,\n",
    "  min_instances=1000, clock=500, name='target_drift_eval'\n",
    ")\n",
    "\n",
    "# Input Drift Handler (Data + Target Drift + Retraining Strategy)\n",
    "input_monitoring = ModelMonitoringPipeline([(ev1._name, ev1), (ev2._name, ev2)])\n",
    "input_drift_retraining_strategy = DummyRetrainingStrategy(\n",
    "  n_last_retrain=500, n_last_test=0\n",
    ")\n",
    "input_drift_handlers = [\n",
    "  DriftHandler(input_monitoring, input_drift_retraining_strategy)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StreamExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='rf_exp_all_drifts')\n",
    "rf_model = Pipeline([('rf', RandomForestClassifier(max_depth=10))])\n",
    "sklearn_batch_classifier = SklearnEstimator(rf_model)\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=1500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='lr_exp_all_drifts')\n",
    "lr_model = Pipeline([('lr', LogisticRegression())])\n",
    "sklearn_batch_classifier = SklearnEstimator(lr_model)\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=1500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='nb_exp_all_drifts')\n",
    "nb_model = Pipeline([('nb', GaussianNB())])\n",
    "sklearn_batch_classifier = SklearnEstimator(nb_model)\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=1500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "logger_factory = LoggerFactory(experiment_id='xgb_exp_all_drifts')\n",
    "xgb_model = Pipeline([('xgb', XGBClassifier())])\n",
    "sklearn_batch_classifier = SklearnEstimator(xgb_model)\n",
    "batch_pipeline = BatchPipeline(\n",
    "    sklearn_batch_classifier,\n",
    "    input_drift_handlers,\n",
    "    output_drift_handlers,\n",
    "    history,\n",
    "    logger_factory,\n",
    "    model_comparer,\n",
    "    min_samples_retrain=1500,\n",
    "    min_samples_first_fit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'1': [1, 2, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dataset'] = 'x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = range(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO, TextIOWrapper\n",
    "from zipfile import ZipFile\n",
    "from scipy.io.arff import loadarff\n",
    "from os import path\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "\n",
    "def get_covtype_dataset(data_path='./data') -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    zip_path = path.join(data_path, 'COVTYPE/covtypeNorm.arff.zip')\n",
    "    with (ZipFile(zip_path, 'r')) as zfile:\n",
    "        in_mem_fo = TextIOWrapper(BytesIO(zfile.read('covtypeNorm.arff')), encoding='ascii')\n",
    "        data = loadarff(in_mem_fo)\n",
    "        df = pd.DataFrame(data[0])\n",
    "        to_convert_df = df.select_dtypes([object])\n",
    "        to_convert_col_names = to_convert_df.columns\n",
    "        df[to_convert_col_names] = to_convert_df.stack().str.decode('utf-8').unstack()\n",
    "        class_col = df.pop('class').replace(['noad', 'ad'], [0, 1])\n",
    "        df['target'] = class_col.copy()\n",
    "        df.loc[:, \"Wilderness_Area1\":] = df.loc[:, \"Wilderness_Area1\":].astype(str).astype(int)\n",
    "        df['dataset'] = 'covtypeNorm'\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_covtype_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from batchstream.utils.visualization import visualize_results\n",
    "from batchstream.utils.reading_logs import load_drift_history, get_metrics_vals\n",
    "\n",
    "\n",
    "\n",
    "out_dir = Path(r'D:\\Studia\\praca\\out\\20230505_1848')\n",
    "dir_list = [f for f in out_dir.resolve().glob('*') if not f.is_file()]\n",
    "for d in dir_list:\n",
    "    drift_hist = load_drift_history(str(d))\n",
    "    res = get_metrics_vals(str(d))\n",
    "    print(str(d))\n",
    "    visualize_results(res, drift_hist, dataset_name='COVTYPE', metrics=['acc'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift features nie występuje\n",
    "# Drift perfromance też nie (?) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from batchstream.utils.visualization import visualize_results\n",
    "from batchstream.utils.reading_logs import load_drift_history, get_metrics_vals\n",
    "\n",
    "\n",
    "\n",
    "out_dir = Path(r'D:\\Studia\\praca\\out\\20230506_1423')\n",
    "dir_list = [f for f in out_dir.resolve().glob('*') if not f.is_file()]\n",
    "for d in dir_list:\n",
    "    drift_hist = load_drift_history(str(d))\n",
    "    res = get_metrics_vals(str(d))\n",
    "    print(str(d))\n",
    "    visualize_results(res, drift_hist, dataset_name='COVTYPE', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(res, drift_hist, dataset_name='COVTYPE', metrics=['kappa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.tests import *\n",
    "from batchstream.history.base.history_manager import HistoryManager\n",
    "from batchstream.monitoring.pipeline.drift_monitoring_pipeline import DriftMonitoringPipeline\n",
    "from batchstream.monitoring.pipeline.steps.batch.evidently_monitoring_step import EvidentlyMonitoringStep\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "from batchstream.batch_monitoring_strategy.simple_monitoring_strategy import SimpleMonitoringStrategy\n",
    "from batchstream.retraining_strategy.simple_retraining_strategy import SimpleRetrainingStrategy \n",
    "from batchstream.drift_handlers.base.drift_handler import DriftHandler\n",
    "from tqdm import tqdm\n",
    "from river import stream\n",
    "from utils.read_data.get_dataset import get_dataset\n",
    "\n",
    "\n",
    "\n",
    "n_start_detect=2_500\n",
    "n_curr = 2_500\n",
    "n_ref = 2_500\n",
    "stattest = None\n",
    "stattest_threshold = 0.04\n",
    "dataset = 'rbf0.66_3'\n",
    "drift_share = None\n",
    "\n",
    "\n",
    "history = HistoryManager()\n",
    "name = f'{dataset}_data_drift_{stattest}_{stattest_threshold}_{n_curr}_{n_ref}_{drift_share}'\n",
    "logger_factory = LoggerFactory(name)\n",
    "\n",
    "data_drift_test_suite_args = {\n",
    "    'tests': [DataDriftTestPreset(stattest_threshold=stattest_threshold)]\n",
    "}\n",
    "d1 = SimpleMonitoringStrategy(n_curr=n_curr, n_ref=n_ref)\n",
    "ev1 = EvidentlyMonitoringStep(data_drift_test_suite_args, d1, logger_factory, min_instances=n_curr, clock=n_curr, name=name)\n",
    "\n",
    "input_monitoring = DriftMonitoringPipeline([(ev1._name, ev1)])\n",
    "input_drift_retraining_strategy = SimpleRetrainingStrategy(n_last_retrain=n_curr, n_last_test=0)\n",
    "input_detector = DriftHandler(input_monitoring, input_drift_retraining_strategy)\n",
    "\n",
    "#df = get_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchstream.monitoring.pipeline.steps.online.river_monitoring_step import RiverMonitoringStep\n",
    "from river import drift\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.tests import *\n",
    "from batchstream.history.base.history_manager import HistoryManager\n",
    "from batchstream.monitoring.pipeline.drift_monitoring_pipeline import DriftMonitoringPipeline\n",
    "from batchstream.monitoring.pipeline.steps.batch.evidently_monitoring_step import EvidentlyMonitoringStep\n",
    "from batchstream.utils.logging.base.logger_factory import LoggerFactory\n",
    "from batchstream.batch_monitoring_strategy.simple_monitoring_strategy import SimpleMonitoringStrategy\n",
    "from batchstream.retraining_strategy.simple_retraining_strategy import SimpleRetrainingStrategy \n",
    "from batchstream.drift_handlers.base.drift_handler import DriftHandler\n",
    "from tqdm import tqdm\n",
    "from river import stream\n",
    "from utils.read_data.get_dataset import get_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = 'rbf_adwinx'\n",
    "clock = 1500\n",
    "grace_period = 100\n",
    "min_window_length = 500\n",
    "\n",
    "n_start_detect = grace_period\n",
    "\n",
    "history = HistoryManager()\n",
    "name = f'{dataset}_adwin_data_drift_{clock}_{grace_period}_{min_window_length}'\n",
    "logger_factory = LoggerFactory(name)\n",
    "\n",
    "adwins = []\n",
    "j = 0\n",
    "for col in [0, 1, 2, 3]:\n",
    "    if col == 'dataset': continue\n",
    "    if col == 'target': continue\n",
    "    adwin = RiverMonitoringStep(col, j, drift.ADWIN(clock=clock, grace_period=grace_period, min_window_length=min_window_length), logger_factory)\n",
    "    adwins.append(adwin)\n",
    "    j += 1\n",
    "\n",
    "input_monitoring = DriftMonitoringPipeline([(a._name, a) for a in adwins])\n",
    "input_drift_retraining_strategy = SimpleRetrainingStrategy(n_last_retrain=clock, n_last_test=0)\n",
    "input_detector = DriftHandler(input_monitoring, input_drift_retraining_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets import synth\n",
    "\n",
    "\n",
    "dataset = synth.RandomRBFDrift(seed_model=42, seed_sample=42,\n",
    "    n_classes=4, n_features=4, n_centroids=20,\n",
    "    change_speed=0.66, n_drift_centroids=10)\n",
    "\n",
    "i = 0\n",
    "for x, y in tqdm(dataset.take(250_000)):\n",
    "    history.update_history_x(x)\n",
    "    if i >= n_start_detect:\n",
    "        drift_detected = input_detector.detect(history)\n",
    "    history.update_history_y(y)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets import synth\n",
    "\n",
    "dataset = synth.RandomRBFDrift(seed_model=42, seed_sample=42,\n",
    "    n_classes=4, n_features=4, n_centroids=20,\n",
    "    change_speed=0.66, n_drift_centroids=10)\n",
    "\n",
    "adwin = drift.ADWIN(clock=1500, min_window_length=250, grace_period=100)\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "for x, y in tqdm(dataset.take(250_000)):\n",
    "    val = x[0]\n",
    "    adwin.update(val)\n",
    "    if adwin.drift_detected:\n",
    "        print(f\"Change detected at index {i}, input value: {val}\")\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = HistoryManager()\n",
    "for x, y in tqdm(dataset.take(250_000)):  \n",
    "    history.update_history_x(x)\n",
    "    val_x = history.x_history.iloc[0, -1]\n",
    "    history.update_history_y(y)\n",
    "    val_y = history.y_history.iloc[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchstream.history.base.history_manager2 import HistoryManager2\n",
    "\n",
    "history = HistoryManager2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets import synth\n",
    "\n",
    "dataset = synth.RandomRBFDrift(seed_model=42, seed_sample=42,\n",
    "    n_classes=4, n_features=4, n_centroids=20,\n",
    "    change_speed=0.66, n_drift_centroids=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x, y in dataset.take(250_000):  \n",
    "    history.update_history_x(x)\n",
    "    val_x = history.x_history.iloc[0, -1]\n",
    "    history.update_history_y(y)\n",
    "    val_y = history.y_history.iloc[-1]\n",
    "    i +=1\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
