{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja odpalana jako pandas_udf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class RetrainingStrategy(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_retraining_data(x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class EvaluationStrategyManager(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_curr_ref_data(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ModelEstimatorPipeline(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def handle(self, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_name(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "\n",
    "class ModelSklearnPipeline(ModelEstimatorPipeline):\n",
    "\n",
    "    def __init__(self, sklearn_pipeline: Pipeline, hyperparameter_space: Dict, retraining_strategy: TrainingStrategyManager):\n",
    "        self.estimator = sklearn_pipeline\n",
    "        self.hyperparameter_space = hyperparameter_space\n",
    "        self.retraining_strategy = retraining_strategy\n",
    "\n",
    "    def handle(self, x, y):\n",
    "        return self.estimator.predict(x)\n",
    "    \n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        x_train, y_train = self.retraining_strategy.get_retraining_data(x_history, y_history, prediction_history, drift_history)\n",
    "        self.estimator.fit(x_train, y_train) \n",
    "\n",
    "    def get_name(self):\n",
    "        return super().get_name() # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ModelRiverPipeline(ModelEstimatorPipeline):\n",
    "    \n",
    "    def __init__(self, river_pipeline: compose.Pipeline):\n",
    "        self.estimator = river_pipeline\n",
    "\n",
    "    def handle(self, x, y):\n",
    "        prediction = self.estimator.predict_one(x)\n",
    "        self.estimator.learn_one(x, y)\n",
    "        return prediction\n",
    "\n",
    "    def adjust_model(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]):\n",
    "        return\n",
    "    \n",
    "    def get_name(self):\n",
    "        return super().get_name() # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationPipeline:\n",
    "    \n",
    "    def __init__(self, metric_steps):\n",
    "        self.metric_steps = metric_steps\n",
    "\n",
    "    def handle(self, y_true, y_predict):\n",
    "        results = {}\n",
    "        for metric_name, metric in self.steps:\n",
    "            metric_value = metric.update(y_true, y_predict)\n",
    "            results.update({metric_name: metric_value})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class MonitoringStep(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def monitor(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]) -> bool:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.test_suite import TestSuite\n",
    "\n",
    "\n",
    "\n",
    "class EvidentlyMonitoringStep(MonitoringStep):\n",
    "\n",
    "    def __init__(self, evidently_test_suite: TestSuite, evaluation_strategy: EvaluationStrategyManager):\n",
    "        self.detector = evidently_test_suite\n",
    "        self.eval_strategy = evaluation_strategy\n",
    "\n",
    "    def monitor(self, x_history: List, y_history: List[int], prediction_history: List[int], drift_history: List[int]) -> bool:\n",
    "        curr, ref = self.eval_strategy.get_curr_ref_data(x_history, y_history, prediction_history, drift_history)\n",
    "        self.detector.run(reference_data=ref,current_data=curr)\n",
    "        report = self.detector.as_dict()\n",
    "        return True # to do based on report\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment -> uruchomiony na danym partition: definiuje experiment pipeline\n",
    "\n",
    "\n",
    "\n",
    "datastream_name = datastream['name'][0]\n",
    "datastream = datastream.drop(col=['name'])\n",
    "logger = Logger(dataset_name=name)\n",
    "\n",
    "logger.start()\n",
    "for x, y in datastream:\n",
    "\n",
    "   logger.iter()\n",
    "   pipe = StreamClassificationPipeline()\n",
    "\n",
    "   pipe.handle(x, y)\n",
    "   logger.iter_end()\n",
    "\n",
    "logger.end()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list([{'a': 4}][0].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.drift import ADWIN\n",
    "\n",
    "a = ADWIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "river.drift.adwin.ADWIN"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_helper'.startswith('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('_drift_detected', False), ('delta', 0.002), ('clock', 32), ('max_buckets', 5), ('min_window_length', 5), ('grace_period', 10), ('_helper', <river.drift.adwin_c.AdaptiveWindowing object at 0x000001BA0C9EA5C0>)])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(a).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('clock', 32),\n",
       " ('delta', 0.002),\n",
       " ('grace_period', 10),\n",
       " ('max_buckets', 5),\n",
       " ('min_window_length', 5)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{item for item in vars(a).items() if not item[0].startswith('_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, ensemble, model_selection\n",
    "\n",
    "from evidently import ColumnMapping\n",
    "from evidently.test_suite import TestSuite\n",
    "\n",
    "from evidently.test_preset import NoTargetPerformanceTestPreset\n",
    "from evidently.test_preset import DataQualityTestPreset\n",
    "from evidently.test_preset import DataStabilityTestPreset\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.test_preset import RegressionTestPreset\n",
    "from evidently.test_preset import MulticlassClassificationTestPreset\n",
    "from evidently.test_preset import BinaryClassificationTopKTestPreset\n",
    "from evidently.test_preset import BinaryClassificationTestPreset\n",
    "\n",
    "from evidently.tests import TestNumberOfEmptyRows, TestNumberOfEmptyColumns, TestNumberOfDuplicatedRows, TestNumberOfDuplicatedColumns, TestNumberOfDriftedColumns, TestShareOfDriftedColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\golik\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#Dataset for Data Quality and Integrity\n",
    "adult_data = datasets.fetch_openml(name='adult', version=2, as_frame='auto')\n",
    "adult = adult_data.frame\n",
    "\n",
    "adult_ref = adult[~adult.education.isin(['Some-college', 'HS-grad', 'Bachelors'])]\n",
    "adult_cur = adult[adult.education.isin(['Some-college', 'HS-grad', 'Bachelors'])]\n",
    "\n",
    "adult_cur.iloc[:2000, 3:5] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for Regression\n",
    "housing_data = datasets.fetch_california_housing(as_frame='auto')\n",
    "housing = housing_data.frame\n",
    "\n",
    "housing.rename(columns={'MedHouseVal': 'target'}, inplace=True)\n",
    "housing['prediction'] = housing_data['target'].values + np.random.normal(0, 3, housing.shape[0])\n",
    "\n",
    "housing_ref = housing.sample(n=5000, replace=False)\n",
    "housing_cur = housing.sample(n=5000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for Binary Probabilistic Classifcation\n",
    "bcancer_data = datasets.load_breast_cancer(as_frame='auto')\n",
    "bcancer = bcancer_data.frame\n",
    "\n",
    "bcancer_ref = bcancer.sample(n=300, replace=False)\n",
    "bcancer_cur = bcancer.sample(n=200, replace=False)\n",
    "\n",
    "bcancer_label_ref = bcancer_ref.copy(deep=True)\n",
    "bcancer_label_cur = bcancer_cur.copy(deep=True)\n",
    "\n",
    "model = ensemble.RandomForestClassifier(random_state=1, n_estimators=10)\n",
    "model.fit(bcancer_ref[bcancer_data.feature_names.tolist()], bcancer_ref.target)\n",
    "\n",
    "bcancer_ref['prediction'] = model.predict_proba(bcancer_ref[bcancer_data.feature_names.tolist()])[:, 1]\n",
    "bcancer_cur['prediction'] = model.predict_proba(bcancer_cur[bcancer_data.feature_names.tolist()])[:, 1]\n",
    "\n",
    "bcancer_label_ref['prediction'] = model.predict(bcancer_label_ref[bcancer_data.feature_names.tolist()])\n",
    "bcancer_label_cur['prediction'] = model.predict(bcancer_label_cur[bcancer_data.feature_names.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for Multiclass Classifcation\n",
    "iris_data = datasets.load_iris(as_frame='auto')\n",
    "iris = iris_data.frame\n",
    "\n",
    "iris_ref = iris.sample(n=75, replace=False)\n",
    "iris_cur = iris.sample(n=75, replace=False)\n",
    "\n",
    "model = ensemble.RandomForestClassifier(random_state=1, n_estimators=3)\n",
    "model.fit(iris_ref[iris_data.feature_names], iris_ref.target)\n",
    "\n",
    "iris_ref['prediction'] = model.predict(iris_ref[iris_data.feature_names])\n",
    "iris_cur['prediction'] = model.predict(iris_cur[iris_data.feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drift_dataset_tests = TestSuite(tests=[\n",
    "    TestNumberOfEmptyRows(),\n",
    "    TestNumberOfEmptyColumns(),\n",
    "    TestNumberOfDuplicatedRows(),\n",
    "    TestNumberOfDuplicatedColumns(),\n",
    "    TestNumberOfDriftedColumns(),\n",
    "    TestShareOfDriftedColumns(),\n",
    "])\n",
    "\n",
    "data_drift_dataset_tests.run(reference_data=adult_ref, current_data=adult_cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test preset as a python object\n",
    "res = data_drift_dataset_tests.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Number of Empty Rows',\n",
       "  'description': 'Number of Empty Rows is 0. The test threshold is eq=0 ± 1e-12.',\n",
       "  'status': 'SUCCESS',\n",
       "  'group': 'data_integrity',\n",
       "  'parameters': {}},\n",
       " {'name': 'Number of Empty Columns',\n",
       "  'description': 'Number of Empty Columns is 0. The test threshold is lte=0.',\n",
       "  'status': 'SUCCESS',\n",
       "  'group': 'data_integrity',\n",
       "  'parameters': {}},\n",
       " {'name': 'Number of Duplicate Rows',\n",
       "  'description': 'The number of duplicate rows is 38. The test threshold is eq=27 ± 2.7.',\n",
       "  'status': 'FAIL',\n",
       "  'group': 'data_integrity',\n",
       "  'parameters': {'condition': {'eq': 26.955634051571884 ± 2.6955634051571886},\n",
       "   'number_of_duplicated_rows': 38}},\n",
       " {'name': 'Number of Duplicate Columns',\n",
       "  'description': 'The number of duplicate columns is 0. The test threshold is lte=0.',\n",
       "  'status': 'SUCCESS',\n",
       "  'group': 'data_integrity',\n",
       "  'parameters': {'condition': {'lte': 0}, 'number_of_duplicated_columns': 0}},\n",
       " {'name': 'Number of Drifted Features',\n",
       "  'description': 'The drift is detected for 5 out of 15 features. The test threshold is lt=5.',\n",
       "  'status': 'FAIL',\n",
       "  'group': 'data_drift',\n",
       "  'parameters': {'condition': {'lt': 5},\n",
       "   'features': {'age': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.185,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'capital-gain': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.082,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'capital-loss': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.034,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'education-num': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.618,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'fnlwgt': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.024,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'hours-per-week': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.089,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'class': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.034,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'education': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.833,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'marital-status': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.036,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'native-country': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.107,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'occupation': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.153,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'race': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.019,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'relationship': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.031,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'sex': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.019,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'workclass': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.041,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'}}}},\n",
       " {'name': 'Share of Drifted Columns',\n",
       "  'description': 'The drift is detected for 33.3% features (5 out of 15). The test threshold is lt=0.3',\n",
       "  'status': 'FAIL',\n",
       "  'group': 'data_drift',\n",
       "  'parameters': {'condition': {'lt': 0.3},\n",
       "   'features': {'age': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.185,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'capital-gain': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.082,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'capital-loss': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.034,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'education-num': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.618,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'fnlwgt': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.024,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'hours-per-week': {'stattest': 'Wasserstein distance (normed)',\n",
       "     'score': 0.089,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'class': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.034,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'education': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.833,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'marital-status': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.036,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'native-country': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.107,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'occupation': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.153,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Detected'},\n",
       "    'race': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.019,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'relationship': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.031,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'sex': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.019,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'},\n",
       "    'workclass': {'stattest': 'Jensen-Shannon distance',\n",
       "     'score': 0.041,\n",
       "     'threshold': 0.1,\n",
       "     'data_drift': 'Not Detected'}}}}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['tests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_drift_detected = False\n",
    "detection_idx = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drift not detected.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "x = Counter({'SUCCESS': 3, 'FAIL': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x1ba0f7e5060>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    return 1, 2, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, kwargs = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo:\n",
    "\n",
    "    def __init__(self, a: int, b: int, weight: int=4, height: int=3):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.weight = weight\n",
    "        self.height = height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= Foo(a, b, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Foo' object has no attribute 'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f\u001b[39m.\u001b[39;49mc\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Foo' object has no attribute 'c'"
     ]
    }
   ],
   "source": [
    "f.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
